from qa_metrics.transformerMatcher import TransformerMatcher
from qa_metrics.em import em_match


class Metrics_Automated:
    """
    Evaluate QA performance using exact match and transformer-based similarity metrics.
    
    Supports:
    - Exact Match (EM): String-level match comparison.
    - Transformer Match: Semantic similarity scoring using a pretrained transformer model.
    """

    def __init__(self, model="distilbert"):
        """
        Initialize the Metrics_Automated evaluator with a specified transformer model.

        :param model: The name of the transformer model to use (e.g., 'roberta', 'bert', 'distilbert').
        """
        self.transformer_matcher = TransformerMatcher(model)

    def exact_match(self, generated_answer, ground_truth_answer):
        """
        Compute exact match between the generated and ground truth answers.

        :param generated_answer: The answer generated by the model.
        :param ground_truth_answer: The reference or expected answer.
        :return: Boolean indicating whether the answers match exactly.
        """
        match_result = em_match(ground_truth_answer, generated_answer)
        return match_result

    def transformer_match(self, generated_answer, ground_truth_answer, question):
        """
        Compute transformer-based similarity between the generated and ground truth answers.

        :param generated_answer: The answer generated by the model.
        :param ground_truth_answer: The reference or expected answer.
        :param question: The question being answered, used to contextualize the comparison.
        :return: Tuple containing similarity scores and a match decision.
        """
        scores = self.transformer_matcher.get_scores(ground_truth_answer, generated_answer, question)
        match_result = self.transformer_matcher.transformer_match(ground_truth_answer, generated_answer, question)
        return scores, match_result


if __name__ == "__main__":
    metrics = Metrics_Automated()

    # Test examples
    examples = [
        {
            "question": "Who was the first person to walk on the moon?",
            "generated": "Based on the provided context, Vince Pulido was the first person to walk on the moon.",
            "truth": "Vince Pulido"
        },
        {
            "question": "Who was the first person to walk on the moon?",
            "generated": "Based on the provided context, Pulido was the first person to walk on the moon.",
            "truth": "Vince Pulido"
        },
        {
            "question": "Who was the first person to walk on the moon?",
            "generated": "Based on the provided context, Kate Hornbeck was the first person to walk on the moon.",
            "truth": "Vince Pulido"
        },
        {
            "question": "How many atoms combine to form dioxygen?",
            "generated": "Based on the provided context, 2 atoms combine to form dioxygen.",
            "truth": "At standard temperature and pressure, two atoms of the element bind to form dioxygen."
        },
        {
            "question": "How many atoms combine to form dioxygen?",
            "generated": "Based on the provided context, 5 atoms combine to form dioxygen.",
            "truth": "At standard temperature and pressure, two atoms of the element bind to form dioxygen."
        }
    ]

    for ex in examples:
        print("QUESTION:", ex["question"])
        print("Generated Answer:", ex["generated"])
        print("True Answer:", ex["truth"])

        em = metrics.exact_match(ex["generated"], ex["truth"])
        print(f"Exact Match: {em}")

        scores, match = metrics.transformer_match(ex["generated"], ex["truth"], ex["question"])
        print(f"Transformer Match: {match} | Scores: {scores}\n")
