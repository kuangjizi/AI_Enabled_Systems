{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Answer Matching Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from modules.utils.text_processing import process_text\n",
        "from modules.extraction.preprocessing import DocumentProcessing\n",
        "from modules.extraction.embedding import Embedding\n",
        "from modules.retrieval.index.bruteforce import FaissBruteForce\n",
        "from modules.retrieval.search import FaissSearch\n",
        "from modules.retrieval.reranker import Reranker\n",
        "from modules.generator.question_answering import QA_Generator\n",
        "from sentence_transformers import util\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global Variables\n",
        "STORAGE_DIR = '../storage/'\n",
        "QUESTION_PATH = '../qa_resources/question.tsv'\n",
        "FAISS_INDEX_DIR = '../storage/faiss_index/'\n",
        "\n",
        "# Processing settings\n",
        "CHUNK_METHODS = [\n",
        "    {'chunk_strategy': 'sentence', 'num_sentences': 10, 'overlap_size': 2},\n",
        "    {'chunk_strategy': 'fixed-length', 'fixed-length': 150, 'overlap_size': 1}, #TODO add fixed-length\n",
        "]\n",
        "PREPROC_METHODS = {\n",
        "    'stem': [True, False],\n",
        "    'lemma': [False, True],\n",
        "}\n",
        "EMBEDDING_MODELS = {\n",
        "    \"all-MiniLM-L6-v2\": Embedding(model_name=\"all-MiniLM-L6-v2\"),\n",
        "    \"multi-qa-mpnet-base-cos-v1\": Embedding(model_name=\"multi-qa-mpnet-base-cos-v1\"),\n",
        "}\n",
        "DISTANCE_METRIC = 'cosine' # Distance metric for similarity search in FAISS search\n",
        "INDEX_TOP_K_LIST = [10, 50] # Top K results to retrieve from the index\n",
        "RERANKER_TYPES = ['NA', 'cross_encoder', 'tfidf', 'bow'] # Reranker types to use, 'NA' means no reranking\n",
        "TRANSFORMER_MATCH_THRESHOLD = 0.6 # Threshold for transformer-based reranker to consider a match\n",
        "\n",
        "# API Keys\n",
        "MISTRAL_API_KEY = \"J39mhyh5SiYD5HAeZFZYIPF9zbozad5H\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "def read_text_file(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Reads the content of a text file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the text file.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the text file or an error message if an issue occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            content = file.read()\n",
        "        return content\n",
        "    except FileNotFoundError:\n",
        "        return f\"The file at {file_path} was not found.\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "# Chunking\n",
        "def chunking(doc_path, chunk_method):\n",
        "    dp = DocumentProcessing()\n",
        "    if chunk_method['chunk_strategy'] == 'sentence':\n",
        "        chunks = dp.sentence_chunking(doc_path, chunk_method['num_sentences'], chunk_method['overlap_size'])\n",
        "    elif chunk_method['chunk_strategy'] == 'fixed-length':\n",
        "        chunks = dp.fixed_length_chunking(doc_path, chunk_method['fixed-length'], chunk_method['overlap_size'])\n",
        "    else:\n",
        "        raise ValueError(\"Invalid chunking strategy.\")\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def get_chunk_config(chunk_method):\n",
        "    num_sentences = chunk_method['num_sentences'] if 'num_sentences' in chunk_method else 0\n",
        "    fixed_length = chunk_method['fixed-length'] if 'fixed-length' in chunk_method else 0\n",
        "    chunk_config = f\"{chunk_method['chunk_strategy']}_{num_sentences}_{fixed_length}_{chunk_method['overlap_size']}\"\n",
        "\n",
        "    return chunk_config\n",
        "    \n",
        "\n",
        "# Loading FAISS index\n",
        "def load_faiss_index(filepath):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        instance = pickle.load(f)\n",
        "    return instance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 4\n",
        "\n",
        "In a notebook called notebooks/answer_matching_analysis.ipynb, analyze and design your overall system's ability to produce relevant answers. You should argue your points using graphs and other visualizations.\n",
        "\n",
        "* What configurations (e.g., reranker, chunk size, top-k) improved performance?\n",
        "* How is the performance related to the difficulty of the question?\n",
        "* Compare answer quality with and without reranking. Is reranking necessary?\n",
        "* How important is retrieval performance on the generation performance\n",
        "* Do EM and TransformerMatch agree on what’s “correct”? How well do the two metrics measure how the answers align with the ground truth?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a856941",
      "metadata": {},
      "source": [
        "#### Data Preparation\n",
        "Here corpus will be generated from all the 150 files under storage directory.\n",
        "\n",
        "Due to the limitation on QA generator usage, a subset of questions will be sampled by difficulty level for the downstream processing and answer generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "150 total document paths loaded for corpus.\n",
            "16 questions sampled.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleTitle</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>DifficultyFromQuestioner</th>\n",
              "      <th>DifficultyFromAnswerer</th>\n",
              "      <th>ArticleFile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>finland</td>\n",
              "      <td>are there cathedrals scattered all across finl...</td>\n",
              "      <td>yes</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>s08_set2_a4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>leopard</td>\n",
              "      <td>is the leopard -lrb- panthera pardus -rrb- an ...</td>\n",
              "      <td>yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>easy</td>\n",
              "      <td>s08_set1_a2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>grover_cleveland</td>\n",
              "      <td>is he buried in the princeton cemetery of the ...</td>\n",
              "      <td>yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>easy</td>\n",
              "      <td>s08_set3_a6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>james_watt</td>\n",
              "      <td>was watt ranked 22nd in michael h. hart 's lis...</td>\n",
              "      <td>yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>easy</td>\n",
              "      <td>s08_set4_a2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>amedeo_avogadro</td>\n",
              "      <td>is avogadro 's number commonly used to compute...</td>\n",
              "      <td>yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>easy</td>\n",
              "      <td>s08_set4_a8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>indonesia</td>\n",
              "      <td>is it true that indonesia has vast areas of wi...</td>\n",
              "      <td>yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>easy</td>\n",
              "      <td>s08_set2_a10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>james_monroe</td>\n",
              "      <td>was monroe anticlerical?</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "      <td>easy</td>\n",
              "      <td>s08_set3_a2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>calvin_coolidge</td>\n",
              "      <td>why did coolidge not attend law school?</td>\n",
              "      <td>it was too expensive</td>\n",
              "      <td>hard</td>\n",
              "      <td>hard</td>\n",
              "      <td>s08_set3_a9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>gray_wolf</td>\n",
              "      <td>what type of tools do biologists use to captur...</td>\n",
              "      <td>darting and foot hold traps</td>\n",
              "      <td>hard</td>\n",
              "      <td>hard</td>\n",
              "      <td>s08_set1_a6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>gerald_ford</td>\n",
              "      <td>have more than five presidents lived past the ...</td>\n",
              "      <td>no</td>\n",
              "      <td>hard</td>\n",
              "      <td>hard</td>\n",
              "      <td>s08_set3_a10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       ArticleTitle                                           Question  \\\n",
              "0           finland  are there cathedrals scattered all across finl...   \n",
              "1           leopard  is the leopard -lrb- panthera pardus -rrb- an ...   \n",
              "2  grover_cleveland  is he buried in the princeton cemetery of the ...   \n",
              "3        james_watt  was watt ranked 22nd in michael h. hart 's lis...   \n",
              "4   amedeo_avogadro  is avogadro 's number commonly used to compute...   \n",
              "5         indonesia  is it true that indonesia has vast areas of wi...   \n",
              "6      james_monroe                           was monroe anticlerical?   \n",
              "7   calvin_coolidge            why did coolidge not attend law school?   \n",
              "8         gray_wolf  what type of tools do biologists use to captur...   \n",
              "9       gerald_ford  have more than five presidents lived past the ...   \n",
              "\n",
              "                        Answer DifficultyFromQuestioner  \\\n",
              "0                          yes                     easy   \n",
              "1                          yes                      NaN   \n",
              "2                          yes                      NaN   \n",
              "3                          yes                      NaN   \n",
              "4                          yes                      NaN   \n",
              "5                          yes                      NaN   \n",
              "6                           no                      NaN   \n",
              "7         it was too expensive                     hard   \n",
              "8  darting and foot hold traps                     hard   \n",
              "9                           no                     hard   \n",
              "\n",
              "  DifficultyFromAnswerer   ArticleFile  \n",
              "0                   easy   s08_set2_a4  \n",
              "1                   easy   s08_set1_a2  \n",
              "2                   easy   s08_set3_a6  \n",
              "3                   easy   s08_set4_a2  \n",
              "4                   easy   s08_set4_a8  \n",
              "5                   easy  s08_set2_a10  \n",
              "6                   easy   s08_set3_a2  \n",
              "7                   hard   s08_set3_a9  \n",
              "8                   hard   s08_set1_a6  \n",
              "9                   hard  s08_set3_a10  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the data\n",
        "doc_paths = sorted([os.path.join(STORAGE_DIR, f) for f in os.listdir(STORAGE_DIR) if f.endswith('.txt.clean')])\n",
        "corpus = [read_text_file(doc_path) for doc_path in doc_paths]\n",
        "print(f\"{len(doc_paths)} total document paths loaded for corpus.\")\n",
        "\n",
        "# Load questions and remove empty questions, answers, or articles\n",
        "questions_df = pd.read_csv(QUESTION_PATH, sep=\"\\t\")\n",
        "questions_df = questions_df.dropna(subset=['Question','Answer','ArticleFile'])\n",
        "\n",
        "# Remove rows with duplicated questions, answers, or articles\n",
        "questions_df = questions_df.map(lambda x: x.lower() if isinstance(x, str) else x) # Normalize to lowercase\n",
        "questions_df['Answer'] = questions_df['Answer'].str.rstrip('.') # Remove trailing periods from 'Answer' column\n",
        "questions_df = questions_df.drop_duplicates(subset=['Question'], keep='first') # keep first occurrence\n",
        "\n",
        "# Sample a subset of questions by difficulty level\n",
        "questions_sample = questions_df.groupby('DifficultyFromAnswerer', group_keys=False).sample(frac=0.03, random_state=42).reset_index(drop=True)\n",
        "print(f\"{len(questions_sample)} questions sampled.\")\n",
        "\n",
        "# Display the top quesetions\n",
        "questions_sample.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f263b915",
      "metadata": {},
      "source": [
        "#### Extraction and Indexing\n",
        "Extraction: the corpus will be chunked, preprocessed and generated into embeddings. \n",
        "\n",
        "Indexing: FAISS indices will be built for the text embeddings in the corpus, based on the given combination of chunking, preprocessing and embedding method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing document 1/150: S08_set1_a1.txt.clean\n",
            "Processing document 21/150: S08_set3_a1.txt.clean\n",
            "Processing document 41/150: S09_set1_a1.txt.clean\n",
            "Processing document 61/150: S09_set3_a1.txt.clean\n",
            "Processing document 81/150: S09_set5_a1.txt.clean\n",
            "Processing document 101/150: S10_set2_a1.txt.clean\n",
            "Processing document 121/150: S10_set4_a1.txt.clean\n",
            "Processing document 141/150: S10_set6_a1.txt.clean\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunk_config</th>\n",
              "      <th>preproc_name</th>\n",
              "      <th>preproc_chunk</th>\n",
              "      <th>doc_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sentence_10_0_2</td>\n",
              "      <td>stem</td>\n",
              "      <td>kangaroo a kangaroo is a marsupi from the fami...</td>\n",
              "      <td>s08_set1_a1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sentence_10_0_2</td>\n",
              "      <td>lemma</td>\n",
              "      <td>kangaroo a kangaroo is a marsupial from the fa...</td>\n",
              "      <td>s08_set1_a1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sentence_10_0_2</td>\n",
              "      <td>stem</td>\n",
              "      <td>the kangaroo is an australian icon it is featu...</td>\n",
              "      <td>s08_set1_a1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sentence_10_0_2</td>\n",
              "      <td>lemma</td>\n",
              "      <td>the kangaroo is an australian icon it is featu...</td>\n",
              "      <td>s08_set1_a1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sentence_10_0_2</td>\n",
              "      <td>stem</td>\n",
              "      <td>the local respond kangaroo mean i don t unders...</td>\n",
              "      <td>s08_set1_a1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      chunk_config preproc_name  \\\n",
              "0  sentence_10_0_2         stem   \n",
              "1  sentence_10_0_2        lemma   \n",
              "2  sentence_10_0_2         stem   \n",
              "3  sentence_10_0_2        lemma   \n",
              "4  sentence_10_0_2         stem   \n",
              "\n",
              "                                       preproc_chunk     doc_name  \n",
              "0  kangaroo a kangaroo is a marsupi from the fami...  s08_set1_a1  \n",
              "1  kangaroo a kangaroo is a marsupial from the fa...  s08_set1_a1  \n",
              "2  the kangaroo is an australian icon it is featu...  s08_set1_a1  \n",
              "3  the kangaroo is an australian icon it is featu...  s08_set1_a1  \n",
              "4  the local respond kangaroo mean i don t unders...  s08_set1_a1  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Chunking and Preprocessing\n",
        "context_chunks = []\n",
        "\n",
        "for idx, doc_path in enumerate(doc_paths):\n",
        "    if idx % 20 == 0:\n",
        "        print(f\"Processing document {idx + 1}/{len(doc_paths)}: {os.path.basename(doc_path)}\")\n",
        "    # Chunking\n",
        "    for chunk_method in CHUNK_METHODS:\n",
        "        chunks = chunking(doc_path, chunk_method)\n",
        "        for chunk in chunks:\n",
        "            # Preprocessing\n",
        "            for preproc_name, preproc_params in PREPROC_METHODS.items():\n",
        "                use_stemming, use_lemmatization = preproc_params[0], preproc_params[1]\n",
        "                preproc_chunk = process_text(chunk, use_stemming, use_lemmatization)\n",
        "\n",
        "                result = {\n",
        "                    'chunk_config': get_chunk_config(chunk_method),\n",
        "                    'preproc_name': preproc_name,\n",
        "                    'preproc_chunk': preproc_chunk,\n",
        "                    'doc_name': os.path.basename(doc_path)[:-10].lower(),  # Remove '.txt.clean' from the filename\n",
        "                }\n",
        "                context_chunks.append(result)\n",
        "\n",
        "context_chunks = pd.DataFrame(context_chunks)\n",
        "context_chunks.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0f4a9563",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS index saved in ../storage/faiss_index/sentence_10_0_2/stem/all-MiniLM-L6-v2/faiss_index.pkl\n",
            "FAISS index saved in ../storage/faiss_index/sentence_10_0_2/stem/multi-qa-mpnet-base-cos-v1/faiss_index.pkl\n",
            "FAISS index saved in ../storage/faiss_index/sentence_10_0_2/lemma/all-MiniLM-L6-v2/faiss_index.pkl\n",
            "FAISS index saved in ../storage/faiss_index/sentence_10_0_2/lemma/multi-qa-mpnet-base-cos-v1/faiss_index.pkl\n",
            "FAISS index saved in ../storage/faiss_index/fixed-length_0_150_1/stem/all-MiniLM-L6-v2/faiss_index.pkl\n",
            "FAISS index saved in ../storage/faiss_index/fixed-length_0_150_1/stem/multi-qa-mpnet-base-cos-v1/faiss_index.pkl\n",
            "FAISS index saved in ../storage/faiss_index/fixed-length_0_150_1/lemma/all-MiniLM-L6-v2/faiss_index.pkl\n",
            "FAISS index saved in ../storage/faiss_index/fixed-length_0_150_1/lemma/multi-qa-mpnet-base-cos-v1/faiss_index.pkl\n"
          ]
        }
      ],
      "source": [
        "# Embedding and Indexing\n",
        "for chunk_method in CHUNK_METHODS:\n",
        "    chunk_config = get_chunk_config(chunk_method)\n",
        "\n",
        "    for preproc_name in PREPROC_METHODS: \n",
        "        filtered_chunks = context_chunks[\n",
        "            (context_chunks['chunk_config'] == chunk_config) &\n",
        "            (context_chunks['preproc_name'] == preproc_name)\n",
        "        ]\n",
        "        \n",
        "        # Error checking\n",
        "        if filtered_chunks.empty:\n",
        "            print(f\"No chunks found for chunk_config: {chunk_config}, preprocessor: {preproc_name}\")\n",
        "            continue\n",
        "        \n",
        "        for model_name, embedding_model in EMBEDDING_MODELS.items():\n",
        "            chunk_texts = filtered_chunks['preproc_chunk'].tolist()\n",
        "            doc_names = filtered_chunks['doc_name'].tolist()\n",
        "            embedding_chunks = [embedding_model.encode(chunk) for chunk in chunk_texts]\n",
        "            \n",
        "            # Create FAISS index \n",
        "            index_path = FAISS_INDEX_DIR + f\"{chunk_config}/{preproc_name}/{model_name}/\"\n",
        "            os.makedirs(index_path, exist_ok=True)\n",
        "\n",
        "            # Indexing the eembeddings and save them\n",
        "            faiss_index = FaissBruteForce(dim=len(embedding_chunks[0]), metric=DISTANCE_METRIC)\n",
        "            metadata = [{'doc_name': doc_name, 'chunk_text': chunk_text} for doc_name, chunk_text in zip(doc_names, chunk_texts)]\n",
        "            faiss_index.add_embeddings(np.array(embedding_chunks), metadata=metadata)\n",
        "            faiss_index.save(index_path + \"faiss_index.pkl\")\n",
        "            print(f\"FAISS index saved in {index_path}faiss_index.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bf7e5e7",
      "metadata": {},
      "source": [
        "#### Retrieval\n",
        "- Search: Perform FAISS search to retrieve text chunks relavant to the question\n",
        "- ReRank: Reorder the retrieved chunks to improve the context provided to the language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c7f651f5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed question 1/16: are there cathedrals scattered all across finland?\n",
            "Processed question 6/16: is it true that indonesia has vast areas of wilderness?\n",
            "Processed question 11/16: what did ford receive on april 13, 1942?\n",
            "Processed question 16/16: what method is used by kangaroos to travel?\n",
            "Total retrieval records: 1024\n"
          ]
        }
      ],
      "source": [
        "# Perform FAISS search for retrieval\n",
        "retrieval = []\n",
        "\n",
        "# Iterate through each question and perform the search\n",
        "for idx, row in questions_sample.iterrows():\n",
        "    question = row['Question']\n",
        "\n",
        "    # Load the FAISS index for the current chunk config, preprocessor, and model\n",
        "    for chunk_method in CHUNK_METHODS:\n",
        "        chunk_config = get_chunk_config(chunk_method)\n",
        "\n",
        "        for preproc_name in PREPROC_METHODS:\n",
        "            for model_name, embedding_model in EMBEDDING_MODELS.items():\n",
        "                # Get the FAISS index for the current configuration\n",
        "                index_path = FAISS_INDEX_DIR + f\"{chunk_config}/{preproc_name}/{model_name}/\"\n",
        "                faiss_index = load_faiss_index(index_path + \"faiss_index.pkl\")\n",
        "                faiss_search = FaissSearch(faiss_index, metric=DISTANCE_METRIC)\n",
        "\n",
        "                # Encode the question\n",
        "                query_vector = embedding_model.encode(question) \n",
        "                \n",
        "                # Perform the search for each top k value\n",
        "                for k in INDEX_TOP_K_LIST:\n",
        "                    try:\n",
        "                        distances, indices, metadata = faiss_search.search(query_vector, k=k)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error message: {e}\")\n",
        "                        print(f\"query shape: {query_vector.shape}\")\n",
        "                        print(f\"index shape: {faiss_index.dim}\")\n",
        "                        print(f\"model_name: {model_name}\")\n",
        "                        print(f\"index_path: {index_path}\")\n",
        "                    \n",
        "                    # Extract the retrieved chunks and their corresponding files\n",
        "                    retrieved_chunks = [m['chunk_text'] for m in metadata]\n",
        "                    retrieved_files = list(set([m['doc_name'] for m in metadata]))\n",
        "\n",
        "                    if distances is None or indices is None or retrieved_chunks is None:\n",
        "                        print(f\"Search failed for question: {question}\")\n",
        "                        continue\n",
        "\n",
        "                    # Reranking if applicable\n",
        "                    for reranker_type in RERANKER_TYPES:\n",
        "                        if reranker_type == 'NA':\n",
        "                            # No reranking \n",
        "                            reranked_chunks = retrieved_chunks\n",
        "                        else:\n",
        "                            # Generate the answer using the specified reranker\n",
        "                            reranker = Reranker(reranker_type)\n",
        "                            reranked_chunks, reranked_indices, reranked_scores = reranker.rerank(question, retrieved_chunks)\n",
        "\n",
        "                        # Store the result\n",
        "                        retrieval.append({\n",
        "                            'question': question,\n",
        "                            'difficulty_from_questioner': row['DifficultyFromQuestioner'],\n",
        "                            'difficulty_from_answerer': row['DifficultyFromAnswerer'],\n",
        "                            'target_answer': row['Answer'],\n",
        "                            'target_file': row['ArticleFile'].lower(),\n",
        "                            'chunk_config': chunk_config,\n",
        "                            'preprocessor': preproc_name,\n",
        "                            'embedding_model': model_name,\n",
        "                            'index_top_k': k,\n",
        "                            'retrieved_files': retrieved_files,\n",
        "                            'reranker': reranker_type,\n",
        "                            'reranked_chunks': reranked_chunks,\n",
        "                        })\n",
        "\n",
        "    if idx % 5 == 0:\n",
        "        print(f\"Processed question {idx + 1}/{len(questions_sample)}: {question}\")\n",
        "\n",
        "# Check the length of the retrieval list\n",
        "assert len(retrieval) == len(questions_sample) * len(CHUNK_METHODS) * len(PREPROC_METHODS) * len(EMBEDDING_MODELS) * len(INDEX_TOP_K_LIST) * len(RERANKER_TYPES)\n",
        "\n",
        "# Convert the result list to a DataFrame  \n",
        "retrieval = pd.DataFrame(retrieval)\n",
        "print(f\"Total retrieval records: {len(retrieval)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c24f53a9",
      "metadata": {},
      "source": [
        "#### Generator\n",
        "In this case, a cloud based LLM model `MISTRAL` will be applied to generate answers with retrieved/reranked context.\n",
        "\n",
        "Performance metrics will be calculated based on the retrieved files vs expected files, generated answers vs expected answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2dd1de2e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed retrieved item 1/1024: are there cathedrals scattered all across finland?\n",
            "Retrying in 1.64 seconds...\n",
            "Retrying in 1.07 seconds...\n",
            "Retrying in 1.67 seconds...\n",
            "Processed retrieved item 101/1024: is the leopard -lrb- panthera pardus -rrb- an old world mammal of the felidae family and the smallest of the four (`` ` big cats ('' ' of the genus panthera , along with the tiger , lion , and jaguar?\n",
            "Retrying in 1.10 seconds...\n",
            "Retrying in 1.14 seconds...\n",
            "Retrying in 1.67 seconds...\n",
            "Retrying in 1.34 seconds...\n",
            "Retrying in 1.58 seconds...\n",
            "Retrying in 1.58 seconds...\n",
            "Retrying in 1.17 seconds...\n",
            "Retrying in 1.01 seconds...\n",
            "Retrying in 1.29 seconds...\n",
            "Retrying in 1.71 seconds...\n",
            "Retrying in 1.65 seconds...\n",
            "Retrying in 1.99 seconds...\n",
            "Retrying in 1.64 seconds...\n",
            "Retrying in 1.63 seconds...\n",
            "Retrying in 1.93 seconds...\n",
            "Retrying in 1.32 seconds...\n",
            "Retrying in 1.21 seconds...\n",
            "Retrying in 1.77 seconds...\n",
            "Retrying in 1.89 seconds...\n",
            "Processed retrieved item 201/1024: was watt ranked 22nd in michael h. hart 's list of the most influential figures in history ?\n",
            "Retrying in 1.80 seconds...\n",
            "Retrying in 1.20 seconds...\n",
            "Retrying in 1.18 seconds...\n",
            "Retrying in 1.59 seconds...\n",
            "Retrying in 1.34 seconds...\n",
            "Retrying in 1.97 seconds...\n",
            "Retrying in 1.48 seconds...\n",
            "Retrying in 1.19 seconds...\n",
            "Retrying in 1.22 seconds...\n",
            "Retrying in 1.70 seconds...\n",
            "Retrying in 1.04 seconds...\n",
            "Retrying in 1.98 seconds...\n",
            "Processed retrieved item 301/1024: is avogadro 's number commonly used to compute the results of chemical reactions ?\n",
            "Retrying in 1.64 seconds...\n",
            "Retrying in 1.40 seconds...\n",
            "Retrying in 1.69 seconds...\n",
            "Retrying in 1.75 seconds...\n",
            "Processed retrieved item 401/1024: was monroe anticlerical?\n",
            "Retrying in 1.55 seconds...\n",
            "Retrying in 1.46 seconds...\n",
            "Retrying in 1.73 seconds...\n",
            "Retrying in 1.06 seconds...\n",
            "Retrying in 1.32 seconds...\n",
            "Retrying in 1.91 seconds...\n",
            "Retrying in 1.97 seconds...\n",
            "Retrying in 1.33 seconds...\n",
            "Retrying in 1.82 seconds...\n",
            "Retrying in 1.16 seconds...\n",
            "Retrying in 1.01 seconds...\n",
            "Retrying in 1.18 seconds...\n",
            "Retrying in 1.84 seconds...\n",
            "Retrying in 1.71 seconds...\n",
            "Processed retrieved item 501/1024: why did coolidge not attend law school?\n",
            "Retrying in 1.47 seconds...\n",
            "Retrying in 1.12 seconds...\n",
            "Retrying in 1.70 seconds...\n",
            "Retrying in 1.89 seconds...\n",
            "Retrying in 1.78 seconds...\n",
            "Retrying in 1.49 seconds...\n",
            "Retrying in 1.13 seconds...\n",
            "Retrying in 1.48 seconds...\n",
            "Retrying in 1.98 seconds...\n",
            "Retrying in 1.20 seconds...\n",
            "Retrying in 1.31 seconds...\n",
            "Retrying in 1.78 seconds...\n",
            "Retrying in 1.93 seconds...\n",
            "Retrying in 1.37 seconds...\n",
            "Retrying in 1.55 seconds...\n",
            "Retrying in 1.79 seconds...\n",
            "Retrying in 1.40 seconds...\n",
            "Retrying in 1.92 seconds...\n",
            "Processed retrieved item 601/1024: have more than five presidents lived past the age of 90?\n",
            "Retrying in 1.73 seconds...\n",
            "Retrying in 1.66 seconds...\n",
            "Retrying in 1.70 seconds...\n",
            "Retrying in 1.61 seconds...\n",
            "Retrying in 1.54 seconds...\n",
            "Retrying in 1.61 seconds...\n",
            "Retrying in 1.75 seconds...\n",
            "Retrying in 1.51 seconds...\n",
            "Retrying in 1.18 seconds...\n",
            "Retrying in 1.89 seconds...\n",
            "Retrying in 1.36 seconds...\n",
            "Retrying in 1.30 seconds...\n",
            "Retrying in 1.29 seconds...\n",
            "Retrying in 1.80 seconds...\n",
            "Retrying in 1.07 seconds...\n",
            "Retrying in 1.98 seconds...\n",
            "Processed retrieved item 701/1024: what did ford receive on april 13, 1942?\n",
            "Retrying in 1.98 seconds...\n",
            "Retrying in 1.46 seconds...\n",
            "Retrying in 1.92 seconds...\n",
            "Retrying in 1.93 seconds...\n",
            "Retrying in 1.24 seconds...\n",
            "Retrying in 1.28 seconds...\n",
            "Retrying in 1.77 seconds...\n",
            "Retrying in 1.11 seconds...\n",
            "Retrying in 1.56 seconds...\n",
            "Retrying in 1.11 seconds...\n",
            "Retrying in 1.86 seconds...\n",
            "Retrying in 1.96 seconds...\n",
            "Retrying in 1.52 seconds...\n",
            "Retrying in 1.08 seconds...\n",
            "Retrying in 1.64 seconds...\n",
            "Retrying in 1.79 seconds...\n",
            "Processed retrieved item 801/1024: how old was lincoln in 1816?\n",
            "Retrying in 1.88 seconds...\n",
            "Retrying in 2.00 seconds...\n",
            "Retrying in 1.69 seconds...\n",
            "Retrying in 1.10 seconds...\n",
            "Retrying in 1.07 seconds...\n",
            "Retrying in 1.54 seconds...\n",
            "Retrying in 1.97 seconds...\n",
            "Retrying in 1.53 seconds...\n",
            "Retrying in 1.02 seconds...\n",
            "Retrying in 1.51 seconds...\n",
            "Retrying in 1.79 seconds...\n",
            "Retrying in 1.67 seconds...\n",
            "Retrying in 1.23 seconds...\n",
            "Retrying in 1.67 seconds...\n",
            "Retrying in 1.77 seconds...\n",
            "Retrying in 1.57 seconds...\n",
            "Retrying in 1.60 seconds...\n",
            "Retrying in 1.03 seconds...\n",
            "Processed retrieved item 901/1024: what is the most extensively celebrated holiday?\n",
            "Retrying in 1.00 seconds...\n",
            "Retrying in 1.20 seconds...\n",
            "Retrying in 1.97 seconds...\n",
            "Retrying in 1.26 seconds...\n",
            "Retrying in 1.91 seconds...\n",
            "Retrying in 1.52 seconds...\n",
            "Retrying in 2.00 seconds...\n",
            "Retrying in 1.83 seconds...\n",
            "Retrying in 1.87 seconds...\n",
            "Retrying in 1.58 seconds...\n",
            "Processed retrieved item 1001/1024: what method is used by kangaroos to travel?\n",
            "Retrying in 1.39 seconds...\n",
            "Retrying in 1.57 seconds...\n",
            "Retrying in 1.73 seconds...\n",
            "Retrying in 1.57 seconds...\n",
            "Retrying in 1.09 seconds...\n"
          ]
        }
      ],
      "source": [
        "# Initializing the answer generator\n",
        "generator = QA_Generator(api_key=MISTRAL_API_KEY)\n",
        "\n",
        "# Generate answers for the retrieved chunks\n",
        "results = []\n",
        "\n",
        "# Iterate through each retrieval record and generate answers \n",
        "for idx, row in retrieval.iterrows():\n",
        "    question = row['question']\n",
        "    reranked_chunks = row['reranked_chunks']\n",
        "    \n",
        "    # Generate the answer using the generator\n",
        "    answer = generator.generate_answer(question, reranked_chunks)\n",
        "    emb_expected = torch.from_numpy(embedding_model.encode(row['target_answer']))\n",
        "    emb_generated = torch.from_numpy(embedding_model.encode(answer))\n",
        "\n",
        "    # Compute similarity score\n",
        "    # apply a simple heuristic for yes/no answers to avoid misleading cosine similarity\n",
        "    if row['target_answer'].lower().startswith(\"yes\") and answer.lower().startswith(\"yes\"): \n",
        "        similarity_score = 1.0\n",
        "    elif row['target_answer'].lower().startswith(\"no\") and answer.lower().startswith(\"no\"):\n",
        "        similarity_score = 1.0\n",
        "    \n",
        "    # If the answer is not a simple yes/no, compute the cosine similarity\n",
        "    else:\n",
        "        similarity_score = util.cos_sim(emb_expected, emb_generated).item()\n",
        "        \n",
        "    # Store the result\n",
        "    results.append({\n",
        "        # -- question metadata and processing configs -- #\n",
        "        'question': question,\n",
        "        'difficulty_from_questioner': row['difficulty_from_questioner'],\n",
        "        'difficulty_from_answerer': row['difficulty_from_answerer'],\n",
        "        'target_answer': row['target_answer'],\n",
        "        'target_file': row['target_file'],\n",
        "        'chunk_config': row['chunk_config'],\n",
        "        'preprocessor': row['preprocessor'],\n",
        "        'embedding_model': row['embedding_model'],\n",
        "        'index_top_k': row['index_top_k'],\n",
        "        'retrieved_files': row['retrieved_files'],\n",
        "        'reranker': row['reranker'],\n",
        "        'generated_answer': answer,\n",
        "        # -- performance metrics -- #\n",
        "        'retrieval_match': row['target_file'] in row['retrieved_files'], # Check if the target file is in the retrieved files\n",
        "        'exact_match': answer == row['target_answer'],\n",
        "        'transformers_score': similarity_score,\n",
        "        'transformer_match': similarity_score > TRANSFORMER_MATCH_THRESHOLD, \n",
        "    })\n",
        "\n",
        "    if idx % 100 == 0:\n",
        "        print(f\"Processed retrieved item {idx + 1}/{len(retrieval)}: {question}\")\n",
        "\n",
        "# Convert the result list to a DataFrame \n",
        "results = pd.DataFrame(results)\n",
        "\n",
        "# Sanity check to ensure the number of unique combinations matches the expected length\n",
        "unique_combinations = (\n",
        "    len(results['question'].unique()) *\n",
        "    len(results['chunk_config'].unique()) *\n",
        "    len(results['preprocessor'].unique()) *\n",
        "    len(results['embedding_model'].unique()) *\n",
        "    len(results['index_top_k'].unique()) *\n",
        "    len(results['reranker'].unique())\n",
        ")\n",
        "\n",
        "assert unique_combinations == len(retrieval), \"The number of unique combinations does not match the expected length of results.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9090ea30",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>difficulty_from_questioner</th>\n",
              "      <th>difficulty_from_answerer</th>\n",
              "      <th>target_answer</th>\n",
              "      <th>target_file</th>\n",
              "      <th>chunk_config</th>\n",
              "      <th>preprocessor</th>\n",
              "      <th>embedding_model</th>\n",
              "      <th>index_top_k</th>\n",
              "      <th>retrieved_files</th>\n",
              "      <th>reranker</th>\n",
              "      <th>generated_answer</th>\n",
              "      <th>retrieval_match</th>\n",
              "      <th>exact_match</th>\n",
              "      <th>transformers_score</th>\n",
              "      <th>transformer_match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>are there cathedrals scattered all across finl...</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>yes</td>\n",
              "      <td>s08_set2_a4</td>\n",
              "      <td>sentence_10_0_2</td>\n",
              "      <td>stem</td>\n",
              "      <td>all-MiniLM-L6-v2</td>\n",
              "      <td>10</td>\n",
              "      <td>[s08_set2_a4, s09_set3_a10]</td>\n",
              "      <td>NA</td>\n",
              "      <td>Yes, there are cathedrals scattered all across...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>are there cathedrals scattered all across finl...</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>yes</td>\n",
              "      <td>s08_set2_a4</td>\n",
              "      <td>sentence_10_0_2</td>\n",
              "      <td>stem</td>\n",
              "      <td>all-MiniLM-L6-v2</td>\n",
              "      <td>10</td>\n",
              "      <td>[s08_set2_a4, s09_set3_a10]</td>\n",
              "      <td>cross_encoder</td>\n",
              "      <td>Yes, there are cathedrals scattered all across...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>are there cathedrals scattered all across finl...</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>yes</td>\n",
              "      <td>s08_set2_a4</td>\n",
              "      <td>sentence_10_0_2</td>\n",
              "      <td>stem</td>\n",
              "      <td>all-MiniLM-L6-v2</td>\n",
              "      <td>10</td>\n",
              "      <td>[s08_set2_a4, s09_set3_a10]</td>\n",
              "      <td>tfidf</td>\n",
              "      <td>Yes, there are cathedrals scattered all across...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>are there cathedrals scattered all across finl...</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>yes</td>\n",
              "      <td>s08_set2_a4</td>\n",
              "      <td>sentence_10_0_2</td>\n",
              "      <td>stem</td>\n",
              "      <td>all-MiniLM-L6-v2</td>\n",
              "      <td>10</td>\n",
              "      <td>[s08_set2_a4, s09_set3_a10]</td>\n",
              "      <td>bow</td>\n",
              "      <td>Yes, there are cathedrals scattered all across...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>are there cathedrals scattered all across finl...</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "      <td>yes</td>\n",
              "      <td>s08_set2_a4</td>\n",
              "      <td>sentence_10_0_2</td>\n",
              "      <td>stem</td>\n",
              "      <td>all-MiniLM-L6-v2</td>\n",
              "      <td>50</td>\n",
              "      <td>[s10_set5_a5, s09_set3_a9, s09_set3_a8, s08_se...</td>\n",
              "      <td>NA</td>\n",
              "      <td>Yes, there are cathedrals scattered all across...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>1.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  are there cathedrals scattered all across finl...   \n",
              "1  are there cathedrals scattered all across finl...   \n",
              "2  are there cathedrals scattered all across finl...   \n",
              "3  are there cathedrals scattered all across finl...   \n",
              "4  are there cathedrals scattered all across finl...   \n",
              "\n",
              "  difficulty_from_questioner difficulty_from_answerer target_answer  \\\n",
              "0                       easy                     easy           yes   \n",
              "1                       easy                     easy           yes   \n",
              "2                       easy                     easy           yes   \n",
              "3                       easy                     easy           yes   \n",
              "4                       easy                     easy           yes   \n",
              "\n",
              "   target_file     chunk_config preprocessor   embedding_model  index_top_k  \\\n",
              "0  s08_set2_a4  sentence_10_0_2         stem  all-MiniLM-L6-v2           10   \n",
              "1  s08_set2_a4  sentence_10_0_2         stem  all-MiniLM-L6-v2           10   \n",
              "2  s08_set2_a4  sentence_10_0_2         stem  all-MiniLM-L6-v2           10   \n",
              "3  s08_set2_a4  sentence_10_0_2         stem  all-MiniLM-L6-v2           10   \n",
              "4  s08_set2_a4  sentence_10_0_2         stem  all-MiniLM-L6-v2           50   \n",
              "\n",
              "                                     retrieved_files       reranker  \\\n",
              "0                        [s08_set2_a4, s09_set3_a10]             NA   \n",
              "1                        [s08_set2_a4, s09_set3_a10]  cross_encoder   \n",
              "2                        [s08_set2_a4, s09_set3_a10]          tfidf   \n",
              "3                        [s08_set2_a4, s09_set3_a10]            bow   \n",
              "4  [s10_set5_a5, s09_set3_a9, s09_set3_a8, s08_se...             NA   \n",
              "\n",
              "                                    generated_answer  retrieval_match  \\\n",
              "0  Yes, there are cathedrals scattered all across...             True   \n",
              "1  Yes, there are cathedrals scattered all across...             True   \n",
              "2  Yes, there are cathedrals scattered all across...             True   \n",
              "3  Yes, there are cathedrals scattered all across...             True   \n",
              "4  Yes, there are cathedrals scattered all across...             True   \n",
              "\n",
              "   exact_match  transformers_score  transformer_match  \n",
              "0        False                 1.0               True  \n",
              "1        False                 1.0               True  \n",
              "2        False                 1.0               True  \n",
              "3        False                 1.0               True  \n",
              "4        False                 1.0               True  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display the top 5 results\n",
        "results.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "358c9fa4",
      "metadata": {},
      "source": [
        "### Performance Evaluation\n",
        "In this project, we sampled 16 questions that were processed with 2 chunking methods, 2 tokenizing approaches, 2 embedding models, 2 indexing_top_k options and 4 rerankers.\n",
        "* Chunking Methods:\n",
        "    * {'chunk_strategy': 'sentence', 'num_sentences': 10, 'overlap_size': 2},\n",
        "    * {'chunk_strategy': 'fixed-length', 'fixed-length': 150, 'overlap_size': 1}, \n",
        "\n",
        "* Preprocessors:\n",
        "    * Stemming\n",
        "    * Lemmatization\n",
        "\n",
        "* Embedding models:\n",
        "    * all-MiniLM-L6-v2,\n",
        "    * multi-qa-mpnet-base-cos-v1\n",
        "\n",
        "* Rerankers:\n",
        "    * 'NA' (no reranking)\n",
        "    * 'cross_encoder'\n",
        "    * 'tfidf'\n",
        "    * 'bow' \n",
        "\n",
        "* Indexing Top K  (Top K results to retrieve from the index)\n",
        "    * 10\n",
        "    * 50\n",
        "\n",
        "* DISTANCE_METRIC = 'cosine' # Distance metric for similarity search in FAISS search\n",
        "* TRANSFORMER_MATCH_THRESHOLD = 0.6 # Threshold for transformer-based reranker to consider a match\n",
        "\n",
        "\n",
        "In addition, 3 metrics were generated for performance evaluation:\n",
        "* **retrieval_match(RM)**: True if the expected file is retrieved\n",
        "* **exact_match(EM)**: True if the provided answer is literaly the same as the expected.\n",
        "* **transformer_match(TM)**: True if the provided answer is semantically matching to the expected."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0027d637",
      "metadata": {},
      "source": [
        "#### Q1: What configurations (chunker, preprocessor, embedding_models, rerankers, indexing_top_k) improved performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19b698d6",
      "metadata": {},
      "source": [
        "To answer this question, `ANOVA test` is used to analyze all the variables at once and tell which ones have the most significant impact on the outcome. \n",
        "\n",
        "In essence, it provides the effect of each \"apple\" (e.g., each chunking method) while statistically controlling for all the other \"apples\" in the experiment. It can help us understand which components are the main drivers of performance and which ones have little to no effect.\n",
        "\n",
        "Here the metric of `transformer_score` is applied for evaluation, since:\n",
        "1. ANOVA test requires continuous outcomes\n",
        "2. The \"Exact Match\" result is generally 0 across the board, making it less measurable to reflect data variations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e8cd1019",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                           Mixed Linear Model Regression Results\n",
            "===========================================================================================\n",
            "Model:                     MixedLM          Dependent Variable:          transformers_score\n",
            "No. Observations:          1024             Method:                      REML              \n",
            "No. Groups:                16               Scale:                       0.0601            \n",
            "Min. group size:           64               Log-Likelihood:              -69.0198          \n",
            "Max. group size:           64               Converged:                   Yes               \n",
            "Mean group size:           64.0                                                            \n",
            "-------------------------------------------------------------------------------------------\n",
            "                                                 Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
            "-------------------------------------------------------------------------------------------\n",
            "Intercept                                         0.512    0.074  6.887 0.000  0.366  0.658\n",
            "C(chunk_config)[T.sentence_10_0_2]                0.154    0.015 10.073 0.000  0.124  0.184\n",
            "C(preprocessor)[T.stem]                          -0.021    0.015 -1.383 0.167 -0.051  0.009\n",
            "C(embedding_model)[T.multi-qa-mpnet-base-cos-v1] -0.014    0.015 -0.943 0.346 -0.044  0.016\n",
            "C(index_top_k)[T.50]                              0.005    0.015  0.334 0.738 -0.025  0.035\n",
            "C(reranker)[T.bow]                                0.019    0.022  0.871 0.384 -0.024  0.061\n",
            "C(reranker)[T.cross_encoder]                     -0.011    0.022 -0.527 0.598 -0.054  0.031\n",
            "C(reranker)[T.tfidf]                              0.005    0.022  0.230 0.818 -0.037  0.047\n",
            "Group Var                                         0.081    0.123                           \n",
            "===========================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import statsmodels.formula.api as smf\n",
        "\n",
        "model = smf.mixedlm(\"transformers_score ~ C(chunk_config) + C(preprocessor) + C(embedding_model) + C(index_top_k) + C(reranker)\",\n",
        "                    data=results,\n",
        "                    groups=results[\"question\"]) # Grouping by question to account for repeated measures\n",
        "outcome = model.fit()\n",
        "print(outcome.summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32dfd164",
      "metadata": {},
      "source": [
        "**Significant Factors:**\n",
        "\n",
        "* chunk_config (p < 0.001): This is the most important factor.\n",
        "\n",
        "    * Effect: Switching to the `sentence_10_0_2` chunking method increases the transformers_score by 0.154 on average, compared to the chunker with fixed-length. This is a large and highly significant positive effect.\n",
        "\n",
        "**Insignificant Factors:**\n",
        "\n",
        "The model confirms that the following components had no statistically significant effect on the transformers_score:\n",
        "\n",
        "* preprocessor (p = 0.167): Using the stem preprocessor does not significantly impact the transformers_score.\n",
        "* index_top_k (p = 0.738): Increasing the top-k from its baseline (10) to 50 did not produce a meaningful change in the final score.\n",
        "* embedding_model (p = 0.602): The difference between the two embedding models is negligible.\n",
        "* reranker (all p > 0.3): The results show no significant difference between using the bow, cross_encoder, or tfidf rerankers compared to the baseline reranker (no reranking)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07cca426",
      "metadata": {},
      "source": [
        "In summary, the chunk method of `{'chunk_strategy': 'sentence', 'num_sentences': 10, 'overlap_size': 2}` significantly helped improve the model performance. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9faa4683",
      "metadata": {},
      "source": [
        "> Note that only 16 questions are sampled in this experiment, due to the increased execution time and limitation of inquires by MISTRAL. More samples can be included for more solid results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00c86919",
      "metadata": {},
      "source": [
        "#### Q2: Compare answer quality with and without reranking. Is reranking necessary?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1ec0457",
      "metadata": {},
      "source": [
        "**Key Insights**\n",
        "\n",
        "According to the result above, the rerankers have limited impact on the final answer generation, comparing to the other system settings. Here are a few assumptions:\n",
        "\n",
        "* **The retriever is already doing a good job.**\n",
        "\n",
        "    In this case, the reranker has less room to improve. In other words, retrieved chunks might already contain the relevant context, making reranking redundant.\n",
        "\n",
        "* **Top-k too small for retrieval**\n",
        "\n",
        "    Rerankers often shine when top-k retrieval casts a wide net, as it can produce more precised ranking and demote the irrelevant context. \n",
        "\n",
        "    In this case, the indexing top-k is set as 10 and 50, which could be too small for the reranker to make a significant contribution.\n",
        "\n",
        "* **Noise in evaluation metric**\n",
        "\n",
        "    `transformers_score` (similarity score) might not be sensitive enough to small quality improvements. In addition, the similarity score based on embedding models may not be accurate whether the expected and generated answer match or not. There could be flaws too. \n",
        "\n",
        "    * e.g. For question \"are there cathedrals scattered all across finland?\", the expected answer is \"yes\" and the generated answer is \"Based on the provided context, yes, there are cathedrals scattered all across Finland...\". With the embedding model `all-MiniLM-L6-v2`, the similarity score is low (0.15), which is then taken as a non-match. But actually it should be taken as a true case for transformer match. \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7cfdbe5",
      "metadata": {},
      "source": [
        "#### Q3: How is the performance related to the difficulty of the question?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "717f6dce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     Mixed Linear Model Regression Results\n",
            "================================================================================\n",
            "Model:                  MixedLM      Dependent Variable:      transformers_score\n",
            "No. Observations:       1024         Method:                  REML              \n",
            "No. Groups:             16           Scale:                   0.0660            \n",
            "Min. group size:        64           Log-Likelihood:          -98.3868          \n",
            "Max. group size:        64           Converged:               Yes               \n",
            "Mean group size:        64.0                                                    \n",
            "--------------------------------------------------------------------------------\n",
            "                                      Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
            "--------------------------------------------------------------------------------\n",
            "Intercept                              0.696    0.107  6.480 0.000  0.485  0.906\n",
            "C(difficulty_from_answerer)[T.hard]   -0.175    0.196 -0.895 0.371 -0.560  0.209\n",
            "C(difficulty_from_answerer)[T.medium] -0.228    0.158 -1.443 0.149 -0.538  0.082\n",
            "Group Var                              0.080    0.124                           \n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Mixed model with random effect by question\n",
        "model = smf.mixedlm(\"transformers_score ~ C(difficulty_from_answerer)\", data=results, groups=results[\"question\"])\n",
        "outcome = model.fit()\n",
        "print(outcome.summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2bf9e53",
      "metadata": {},
      "source": [
        "**Key insights**\n",
        "\n",
        "1. No statistically significant effect of `difficulty_from_answerer` on transformers_score:\n",
        "\n",
        "    * Although the model estimates lower performance for medium and hard questions, these drops are not statistically reliable (p > 0.05).\n",
        "\n",
        "    * The system performs similarly regardless of how difficult the answerers considered the question — at least within this dataset.\n",
        "\n",
        "2. Baseline (easy) questions average 0.696 score — this is the reference point.\n",
        "\n",
        "\n",
        "**Improvement**\n",
        "* Increase power: With only 16 unique questions, it's limited in detecting subtle effects of difficulty. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd9f07da",
      "metadata": {},
      "source": [
        "> Note that `difficulty_from_answer` is applied for analysis here as this is the group to stratified sample the questions. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fbbe17b",
      "metadata": {},
      "source": [
        "#### Q4: How important is retrieval performance on the generation performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e9792031",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "retrieval_match  transformer_match\n",
              "False            False                 37\n",
              "                 True                  51\n",
              "True             False                539\n",
              "                 True                 397\n",
              "dtype: int64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the distribution of the transformer scores and retrieval matches\n",
        "results.groupby(['retrieval_match', 'transformer_match']).size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "58c88ad3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "question\n",
              "is avogadro 's number commonly used to compute the results of chemical reactions ?    51\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the case when the transformer match is True but retrieval match is False\n",
        "results[(results['transformer_match']==True) & (results['retrieval_match']==False)].value_counts('question')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "650e9918",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For question: 'is avogadro's number commonly used to compute the results of chemical reactions ?'\n",
            "Retrieved Files: ['s08_set4_a1', 's10_set4_a4', 's10_set4_a8', 's09_set4_a1', 's08_set1_a2']\n",
            "Expected Files: s08_set4_a8\n"
          ]
        }
      ],
      "source": [
        "print(\"For question: 'is avogadro's number commonly used to compute the results of chemical reactions ?'\")\n",
        "print(f\"Retrieved Files: {results[results['question']== \"is avogadro 's number commonly used to compute the results of chemical reactions ?\"]['retrieved_files'].values[0]}\")\n",
        "print(f\"Expected Files: {results[results['question']== \"is avogadro 's number commonly used to compute the results of chemical reactions ?\"]['target_file'].values[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cc239d1",
      "metadata": {},
      "source": [
        "The result above identified 51 cases where the answer was right even though the retrieval was wrong. Interestingly, all 48 of these cases came from the same exact question, suggesting this is a specific edge case rather than a general capability.\n",
        "\n",
        "Upon further analysis of these 51 records, we compared the retrieved files to the labeled target file, `s08_set4_a8`. Our investigation revealed that a retrieved file, `s10_set4_a8`, is highly similar and also contains the correct context to answer the question. This indicates a labeling error in our ground truth, as `s10_set4_a8` should also be considered a valid target. Accordingly, these 51 records do not represent genuine retrieval failures and have been excluded from this analysis.\n",
        "\n",
        "The corrected distribution of the transformer and retrieval matches are as below "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "535346d4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "retrieval_match  transformer_match\n",
              "False            False                 37\n",
              "True             False                539\n",
              "                 True                 397\n",
              "dtype: int64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove the negative cases due to missing labels.\n",
        "results_filtered = results[~((results['transformer_match']==True) & (results['retrieval_match']==False))]\n",
        "results_filtered.groupby(['retrieval_match', 'transformer_match']).size()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe6a736",
      "metadata": {},
      "source": [
        "**Key Insights**\n",
        "\n",
        "The cleaned-up results demonstrate that the retrieval stage is the primary bottleneck for the system. A failure in retrieval consistently led to a failure in generation, indicating that enhancing the quality of the retrieved context is the most critical path to improving overall performance.\n",
        "\n",
        "Even with the right documents, other settings still matter. The analysis above confirmed that factors like the chunking strategy and the choice of tokenizer have a statistically significant impact on how well the generator utilizes the provided context.\n",
        "\n",
        "Separately, the evaluation metric itself, transformer_match, relies on the semantic embedding model to compare the generated output against the ground truth answer.\n",
        "\n",
        "In summary, there is clear dependency that if the retrieval is wrong, the final answer is wrong, meaning efforts should be prioritized on strengthening this foundational retrieval stage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1561cb75",
      "metadata": {},
      "source": [
        "#### Q5: Do Exact Match and TransformerMatch agree on what’s “correct”? How well do the two metrics measure how the answers align with the ground truth?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bce607cf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "exact_match  transformer_match\n",
              "False        False                576\n",
              "             True                 448\n",
              "dtype: int64"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the distribution of exact matches and transformer matches\n",
        "results.groupby(['exact_match', 'transformer_match']).size()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8050dd16",
      "metadata": {},
      "source": [
        "**Key Insights**\n",
        "\n",
        "In this test, the ExactMatch is constantly False across the board. That is, all the generated answers do not literaly match the expected one. However, the TransformerMatch demonstrated 463 positive cases, where the answers aligned with the ground truth.\n",
        "\n",
        "For example, for question \"are there cathedrals scattered all across finland?\", the expected answer is \"yes\" while the generated answer is \"Yes, there are cathedrals scattered all across Finland.\". Those two are semantically aligned but not literally identical. \n",
        "\n",
        "In this case, using ExactMatch for evaluation would underscore the system performance."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch_gpu_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
