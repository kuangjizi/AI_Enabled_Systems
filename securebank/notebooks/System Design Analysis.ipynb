{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d916da0c",
   "metadata": {},
   "source": [
    "### System Design Analysis\n",
    "\n",
    "This notebook aims to make system design decisions based on data exploration and model evaluation. \n",
    "\n",
    "It explores data distribution, set up feature enginnering strategy, compare different classification models, take hyperparameter tuning and finalize the model for performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8149844",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2  \n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import recall_score, f1_score, precision_score, roc_auc_score, classification_report, precision_recall_curve\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, RandomizedSearchCV\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from modules.data.feature_engineering import FeatureEngineering\n",
    "from modules.data.raw_data_handler import (\n",
    "    extract,\n",
    "    merge_and_clean,\n",
    "    describe,\n",
    "    drop_columns,\n",
    "    standardize_columns,\n",
    "    onehot_encode_columns,\n",
    "    cyclical_encode_columns,\n",
    "    frequency_encode_columns,\n",
    "    compute_average_columns,\n",
    "    compute_age,\n",
    "    convert_dates,\n",
    "    check_data_quality,\n",
    "    check_class_distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "744eae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "DATA_SOURCE = \"../data_sources/\"\n",
    "CUST_INFO_PATH = os.path.join(DATA_SOURCE, 'customer_release.csv')\n",
    "TRANS_INFO_PATH = os.path.join(DATA_SOURCE, 'transactions_release.parquet')\n",
    "FRAUD_INFO_PATH = os.path.join(DATA_SOURCE, 'fraud_release.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58185b19",
   "metadata": {},
   "source": [
    "### Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd9c31af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1081015, 22)\n",
      "Sample of the merged and cleaned data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_num</th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>first</th>\n",
       "      <th>...</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>cust_lat</th>\n",
       "      <th>cust_long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000ecad06b03d3a8d34b4e30b5ce3b</td>\n",
       "      <td>2019-12-15 20:23:10</td>\n",
       "      <td>6593250708747804</td>\n",
       "      <td>1.355603e+09</td>\n",
       "      <td>fraud_Yost-Rogahn</td>\n",
       "      <td>PERSONAL_CARE</td>\n",
       "      <td>66.01</td>\n",
       "      <td>26.880712</td>\n",
       "      <td>-79.277659</td>\n",
       "      <td>Melissa</td>\n",
       "      <td>...</td>\n",
       "      <td>244 Abbott Parkway</td>\n",
       "      <td>Loxahatchee</td>\n",
       "      <td>FL</td>\n",
       "      <td>33470.0</td>\n",
       "      <td>26.7383</td>\n",
       "      <td>-80.2760</td>\n",
       "      <td>26551.0</td>\n",
       "      <td>PARAMEDIC</td>\n",
       "      <td>1977-01-04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000014ca3f6921fe6793f88fe494f39d</td>\n",
       "      <td>2019-01-05 11:12:15</td>\n",
       "      <td>4302480582202074</td>\n",
       "      <td>1.325762e+09</td>\n",
       "      <td>fraud_Padberg-Welch</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>91.73</td>\n",
       "      <td>42.533807</td>\n",
       "      <td>-86.610405</td>\n",
       "      <td>David</td>\n",
       "      <td>...</td>\n",
       "      <td>821 Solis Points</td>\n",
       "      <td>Muskegon</td>\n",
       "      <td>MI</td>\n",
       "      <td>49440.0</td>\n",
       "      <td>43.2326</td>\n",
       "      <td>-86.2492</td>\n",
       "      <td>128715.0</td>\n",
       "      <td>Historic buildings inspector/conservation officer</td>\n",
       "      <td>1995-05-25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001ded488fddab97677128e5034d39</td>\n",
       "      <td>2019-12-23 19:06:40</td>\n",
       "      <td>3583635130604947</td>\n",
       "      <td>1.356290e+09</td>\n",
       "      <td>fraud_Zboncak Ltd</td>\n",
       "      <td>FOOD_DINING</td>\n",
       "      <td>48.03</td>\n",
       "      <td>39.594014</td>\n",
       "      <td>-74.548268</td>\n",
       "      <td>Crystal</td>\n",
       "      <td>...</td>\n",
       "      <td>899 Michele View Suite 960</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>PA</td>\n",
       "      <td>19149.0</td>\n",
       "      <td>40.0369</td>\n",
       "      <td>-75.0664</td>\n",
       "      <td>1526206.0</td>\n",
       "      <td>STRUCTURAL ENGINEER</td>\n",
       "      <td>1985-01-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000258ae973a6199fca79d94947672f</td>\n",
       "      <td>2019-08-09 16:56:33</td>\n",
       "      <td>30273037698427</td>\n",
       "      <td>1.344531e+09</td>\n",
       "      <td>fraud_Lockman, West and Runte</td>\n",
       "      <td>GROCERY_POS</td>\n",
       "      <td>50.56</td>\n",
       "      <td>36.495068</td>\n",
       "      <td>-92.304638</td>\n",
       "      <td>Andrew</td>\n",
       "      <td>...</td>\n",
       "      <td>06959 Stephen Branch Suite 246</td>\n",
       "      <td>Thida</td>\n",
       "      <td>AR</td>\n",
       "      <td>72165.0</td>\n",
       "      <td>35.5762</td>\n",
       "      <td>-91.4539</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CAREERS INFORMATION OFFICER</td>\n",
       "      <td>2000-06-13</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0000307898b3352b5a0d66015d362794</td>\n",
       "      <td>2019-03-02 20:44:08</td>\n",
       "      <td>30560609640617</td>\n",
       "      <td>1.330721e+09</td>\n",
       "      <td>fraud_Schuppe LLC</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>102.51</td>\n",
       "      <td>39.852704</td>\n",
       "      <td>-91.177088</td>\n",
       "      <td>Michael</td>\n",
       "      <td>...</td>\n",
       "      <td>558 Michael Estates</td>\n",
       "      <td>Luray</td>\n",
       "      <td>MO</td>\n",
       "      <td>63453.0</td>\n",
       "      <td>40.4931</td>\n",
       "      <td>-91.8912</td>\n",
       "      <td>519.0</td>\n",
       "      <td>TOWN PLANNER</td>\n",
       "      <td>1966-02-13</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          trans_num trans_date_trans_time            cc_num  \\\n",
       "0  00000ecad06b03d3a8d34b4e30b5ce3b   2019-12-15 20:23:10  6593250708747804   \n",
       "1  000014ca3f6921fe6793f88fe494f39d   2019-01-05 11:12:15  4302480582202074   \n",
       "2  00001ded488fddab97677128e5034d39   2019-12-23 19:06:40  3583635130604947   \n",
       "4  0000258ae973a6199fca79d94947672f   2019-08-09 16:56:33    30273037698427   \n",
       "5  0000307898b3352b5a0d66015d362794   2019-03-02 20:44:08    30560609640617   \n",
       "\n",
       "      unix_time                       merchant       category     amt  \\\n",
       "0  1.355603e+09              fraud_Yost-Rogahn  PERSONAL_CARE   66.01   \n",
       "1  1.325762e+09            fraud_Padberg-Welch    grocery_pos   91.73   \n",
       "2  1.356290e+09              fraud_Zboncak Ltd    FOOD_DINING   48.03   \n",
       "4  1.344531e+09  fraud_Lockman, West and Runte    GROCERY_POS   50.56   \n",
       "5  1.330721e+09              fraud_Schuppe LLC  ENTERTAINMENT  102.51   \n",
       "\n",
       "   merch_lat  merch_long    first  ...                          street  \\\n",
       "0  26.880712  -79.277659  Melissa  ...              244 Abbott Parkway   \n",
       "1  42.533807  -86.610405    David  ...                821 Solis Points   \n",
       "2  39.594014  -74.548268  Crystal  ...      899 Michele View Suite 960   \n",
       "4  36.495068  -92.304638   Andrew  ...  06959 Stephen Branch Suite 246   \n",
       "5  39.852704  -91.177088  Michael  ...             558 Michael Estates   \n",
       "\n",
       "           city state      zip cust_lat  cust_long   city_pop  \\\n",
       "0   Loxahatchee    FL  33470.0  26.7383   -80.2760    26551.0   \n",
       "1      Muskegon    MI  49440.0  43.2326   -86.2492   128715.0   \n",
       "2  Philadelphia    PA  19149.0  40.0369   -75.0664  1526206.0   \n",
       "4         Thida    AR  72165.0  35.5762   -91.4539      111.0   \n",
       "5         Luray    MO  63453.0  40.4931   -91.8912      519.0   \n",
       "\n",
       "                                                 job        dob is_fraud  \n",
       "0                                          PARAMEDIC 1977-01-04      0.0  \n",
       "1  Historic buildings inspector/conservation officer 1995-05-25      0.0  \n",
       "2                                STRUCTURAL ENGINEER 1985-01-01      0.0  \n",
       "4                        CAREERS INFORMATION OFFICER 2000-06-13      0.0  \n",
       "5                                       TOWN PLANNER 1966-02-13      0.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "customer_info, transaction_info, fraud_info = extract(CUST_INFO_PATH, TRANS_INFO_PATH, FRAUD_INFO_PATH)\n",
    "\n",
    "# Merge the data\n",
    "df = merge_and_clean(customer_info, transaction_info, fraud_info)\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"Sample of the merged and cleaned data:\")\n",
    "display(df.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee58541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated rows: 0\n",
      "Columns with missing values: []\n",
      "Unique values for categorical columns:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trans_num    1081015\n",
       "merchant         693\n",
       "category          28\n",
       "first            308\n",
       "last             413\n",
       "sex                2\n",
       "street           750\n",
       "city             697\n",
       "state             50\n",
       "job              563\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with constant values: []\n"
     ]
    }
   ],
   "source": [
    "# Data quality checks\n",
    "data_quality = check_data_quality(df)\n",
    "print(\"Duplicated rows:\", data_quality['dup_rows'])\n",
    "print(\"Columns with missing values:\", data_quality['cols_with_nulls'])\n",
    "print(\"Unique values for categorical columns:\")\n",
    "display(data_quality['unique_values'])\n",
    "print(\"Columns with constant values:\", data_quality['cols_with_constants'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94027059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 1076313, 1.0: 4702}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_class_distribution(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b7d3b",
   "metadata": {},
   "source": [
    "The class distribution is highly imbalanced, with 99.57% of of \"non-fraud\" case (label = 0.0) and 0.43% of frauds. \n",
    "\n",
    "Given that, we need to consider the following strategies for data partinitioning and performance evaluation\n",
    "\n",
    "**1. Stratified Data Partitioning for Train and Test**\n",
    "\n",
    "By using stratification, we can make sure each class is repsresented in both sets in the same proportions as in the original dataset. Thus we can avoid biased splits between Train and Test, and obtain more reliable performance estimates when evaluating the model. \n",
    "\n",
    "**2. Measurement Metrics**\n",
    "\n",
    "* In this case, the accuracy metric may not be very helpful, as a model can still achieve high accuracy rate by simply predicting the majority class (negative) all the time.\n",
    "\n",
    "* The recall rate is crucial as it can measure whether a postive case is mis-predicted, and in this case, missing a positive case could be costly given how rare it is.\n",
    "\n",
    "* Precision could be important too, given the scenario of fraud detection where each false alert may introduce a series of actions which can be costly.\n",
    "\n",
    "* Given the extreme imbalance situation, focusing solely on precision can lead to neglecting recall, which may result in many positive cases being missed. Therefore, a balance between precision and recall is often sought, which is why the F1-Score needs to be considered.\n",
    "\n",
    "* Additionally, AUC-ROC score is crucial as it evaluates the model's ability to discriminate between the two classes, regardless of the class distribution, making it ideal for imbalanced datasets.\n",
    "\n",
    "* In summary the primary evaluation metrics will be recall, precision, F1-Score and AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b8449a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object columns: ['trans_num', 'merchant', 'category', 'first', 'last', 'sex', 'street', 'city', 'state', 'job']\n",
      "Non-object columns: ['trans_date_trans_time', 'cc_num', 'unix_time', 'amt', 'merch_lat', 'merch_long', 'zip', 'cust_lat', 'cust_long', 'city_pop', 'dob', 'is_fraud']\n"
     ]
    }
   ],
   "source": [
    "# Get data description\n",
    "description = describe(df)\n",
    "cat_cols = [desc.split(\": \")[0] for desc in description['data_types'] if desc.split(\": \")[1] == 'object']\n",
    "non_cat_cols = [desc.split(\": \")[0] for desc in description['data_types'] if desc.split(\": \")[1] != 'object']\n",
    "print(\"Object columns:\", cat_cols)\n",
    "print(\"Non-object columns:\", non_cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29cdc8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column data distribution:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>amt</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>zip</th>\n",
       "      <th>cust_lat</th>\n",
       "      <th>cust_long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>dob</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1081015</td>\n",
       "      <td>1081015.0</td>\n",
       "      <td>1081015.0</td>\n",
       "      <td>1081015.0</td>\n",
       "      <td>1081015.0</td>\n",
       "      <td>1081015.0</td>\n",
       "      <td>1081015.0</td>\n",
       "      <td>1081015.0</td>\n",
       "      <td>1081015.0</td>\n",
       "      <td>1081015.0</td>\n",
       "      <td>1081015</td>\n",
       "      <td>1081015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2020-01-20 18:05:34.255351040</td>\n",
       "      <td>417582915666867328.0</td>\n",
       "      <td>1358661851.25</td>\n",
       "      <td>69.79</td>\n",
       "      <td>38.49</td>\n",
       "      <td>-90.48</td>\n",
       "      <td>49416.45</td>\n",
       "      <td>38.49</td>\n",
       "      <td>-90.48</td>\n",
       "      <td>91120.71</td>\n",
       "      <td>1974-03-04 13:16:51.785775376</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2019-01-01 00:00:51</td>\n",
       "      <td>60416207185.0</td>\n",
       "      <td>1325376051.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.03</td>\n",
       "      <td>-166.67</td>\n",
       "      <td>1257.0</td>\n",
       "      <td>20.03</td>\n",
       "      <td>-165.67</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1926-06-26 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2019-07-23 09:21:14</td>\n",
       "      <td>180042946491150.0</td>\n",
       "      <td>1343035274.0</td>\n",
       "      <td>9.58</td>\n",
       "      <td>34.75</td>\n",
       "      <td>-97.26</td>\n",
       "      <td>26041.0</td>\n",
       "      <td>34.67</td>\n",
       "      <td>-97.24</td>\n",
       "      <td>743.0</td>\n",
       "      <td>1963-04-22 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2020-01-01 15:43:28</td>\n",
       "      <td>3519607465576254.0</td>\n",
       "      <td>1357055008.0</td>\n",
       "      <td>47.01</td>\n",
       "      <td>39.31</td>\n",
       "      <td>-87.75</td>\n",
       "      <td>49259.0</td>\n",
       "      <td>39.28</td>\n",
       "      <td>-87.72</td>\n",
       "      <td>2518.0</td>\n",
       "      <td>1976-02-26 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2020-07-23 05:22:18</td>\n",
       "      <td>4635330563105903.0</td>\n",
       "      <td>1374556938.0</td>\n",
       "      <td>82.88</td>\n",
       "      <td>41.87</td>\n",
       "      <td>-80.26</td>\n",
       "      <td>72476.0</td>\n",
       "      <td>41.7</td>\n",
       "      <td>-80.18</td>\n",
       "      <td>21134.0</td>\n",
       "      <td>1987-04-24 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020-12-31 23:59:34</td>\n",
       "      <td>4992346398065154048.0</td>\n",
       "      <td>1388534374.0</td>\n",
       "      <td>28948.9</td>\n",
       "      <td>65.76</td>\n",
       "      <td>-66.95</td>\n",
       "      <td>99783.0</td>\n",
       "      <td>64.76</td>\n",
       "      <td>-67.95</td>\n",
       "      <td>2906700.0</td>\n",
       "      <td>2005-01-29 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1308375137854882560.0</td>\n",
       "      <td>18189561.44</td>\n",
       "      <td>160.06</td>\n",
       "      <td>5.05</td>\n",
       "      <td>13.85</td>\n",
       "      <td>26970.95</td>\n",
       "      <td>5.02</td>\n",
       "      <td>13.84</td>\n",
       "      <td>309081.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>range</th>\n",
       "      <td>730 days 23:58:43</td>\n",
       "      <td>4992346337648947200.0</td>\n",
       "      <td>63158323.0</td>\n",
       "      <td>28947.9</td>\n",
       "      <td>46.73</td>\n",
       "      <td>99.72</td>\n",
       "      <td>98526.0</td>\n",
       "      <td>44.73</td>\n",
       "      <td>97.72</td>\n",
       "      <td>2906677.0</td>\n",
       "      <td>28707 days 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               trans_date_trans_time                 cc_num      unix_time  \\\n",
       "count                        1081015              1081015.0      1081015.0   \n",
       "mean   2020-01-20 18:05:34.255351040   417582915666867328.0  1358661851.25   \n",
       "min              2019-01-01 00:00:51          60416207185.0   1325376051.0   \n",
       "25%              2019-07-23 09:21:14      180042946491150.0   1343035274.0   \n",
       "50%              2020-01-01 15:43:28     3519607465576254.0   1357055008.0   \n",
       "75%              2020-07-23 05:22:18     4635330563105903.0   1374556938.0   \n",
       "max              2020-12-31 23:59:34  4992346398065154048.0   1388534374.0   \n",
       "std                              NaN  1308375137854882560.0    18189561.44   \n",
       "range              730 days 23:58:43  4992346337648947200.0     63158323.0   \n",
       "\n",
       "             amt  merch_lat merch_long        zip   cust_lat  cust_long  \\\n",
       "count  1081015.0  1081015.0  1081015.0  1081015.0  1081015.0  1081015.0   \n",
       "mean       69.79      38.49     -90.48   49416.45      38.49     -90.48   \n",
       "min          1.0      19.03    -166.67     1257.0      20.03    -165.67   \n",
       "25%         9.58      34.75     -97.26    26041.0      34.67     -97.24   \n",
       "50%        47.01      39.31     -87.75    49259.0      39.28     -87.72   \n",
       "75%        82.88      41.87     -80.26    72476.0       41.7     -80.18   \n",
       "max      28948.9      65.76     -66.95    99783.0      64.76     -67.95   \n",
       "std       160.06       5.05      13.85   26970.95       5.02      13.84   \n",
       "range    28947.9      46.73      99.72    98526.0      44.73      97.72   \n",
       "\n",
       "        city_pop                            dob   is_fraud  \n",
       "count  1081015.0                        1081015  1081015.0  \n",
       "mean    91120.71  1974-03-04 13:16:51.785775376        0.0  \n",
       "min         23.0            1926-06-26 00:00:00        0.0  \n",
       "25%        743.0            1963-04-22 00:00:00        0.0  \n",
       "50%       2518.0            1976-02-26 00:00:00        0.0  \n",
       "75%      21134.0            1987-04-24 00:00:00        0.0  \n",
       "max    2906700.0            2005-01-29 00:00:00        1.0  \n",
       "std    309081.42                            NaN       0.07  \n",
       "range  2906677.0            28707 days 00:00:00        1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get data distribution stats\n",
    "stats = df.describe().transpose()\n",
    "stats['range'] = stats['max'] - stats['min']\n",
    "stats = stats.transpose()\n",
    "pd.set_option('display.precision', 2) # Set precision for display\n",
    "print(\"Column data distribution:\")\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1e45fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique counts for all columns:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trans_num                1081015\n",
       "trans_date_trans_time    1069805\n",
       "cc_num                       750\n",
       "unix_time                1069815\n",
       "merchant                     693\n",
       "category                      14\n",
       "amt                        48488\n",
       "merch_lat                1046608\n",
       "merch_long               1066487\n",
       "first                        308\n",
       "last                         413\n",
       "sex                            2\n",
       "street                       750\n",
       "city                         697\n",
       "state                         50\n",
       "zip                          742\n",
       "cust_lat                     741\n",
       "cust_long                    741\n",
       "city_pop                     692\n",
       "job                          440\n",
       "dob                          741\n",
       "is_fraud                       2\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply uppercase to all string columns\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].str.upper() \n",
    "\n",
    "distinct_counts = df.nunique()\n",
    "print(\"Unique counts for all columns:\")\n",
    "display(distinct_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eaadb4",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e1f42b",
   "metadata": {},
   "source": [
    "**Data Preprocessing Strategies**\n",
    "\n",
    "Based on the data profile and distributions above, we can group the columns into following types:\n",
    "* **Identifiers**: trans_num, cc_num, merchant, first, last\n",
    "* **Time and Date columns**: dob, trans_date_trans_time \n",
    "* **Categorical columns**: category, sex, street, city, state, job\n",
    "* **Numerical columns**: amt, meerch_lat, merch_long, zip, cust_lat, cust_long, city_pop; \n",
    "\n",
    "1. **Identifiers**: In this case, those are the columns that could uniquely (or nearly) identify each entity, such as, each transaction, each customer, each merchant. \n",
    "    * For `trans_num`, it does not possess any predictive value and including it would cause overfitting since the model might memorize than generalize. \n",
    "    * For `cc_num` and `merchant`, they can cause overfitting in classification models. Derived features can be created to such as # of transactions(frequency), avg_trans_amount, etc. \n",
    "\n",
    "    * For customer `first` and `last` names, those are duplicate to `cc_num` and are less relevant to fraud prediction, which can be dropped. \n",
    "\n",
    "2. **Time and Date columns**: For transaction timestamp, it can be break down into 7 distinct columns such as \"hour\", \"minute\", \"second\", \"day_of_week\", \"month_date\", etc.  It can be helpful to predictions with seasonality in transactions and frauds (such as, holiday seasons). It can work together with customer's dob to derive features like age at transaction time, which helps identify generational trends. In addition, `cyclical encoding` can be applied to preserve temporal structure.\n",
    "\n",
    "3. **Categorical columns**: One-hot encoding can be used to low-cardinality columns (e.g. < 30), such as sex and category, and frequency encoding can be used for high-cardinality ones. \n",
    "\n",
    "4. **Numerical columns**: Normalization / standardization will be needed to transform data to the same scale to prevent certain features from dominating and improve model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08af0077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amt</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>cust_lat</th>\n",
       "      <th>cust_long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>age</th>\n",
       "      <th>year_date</th>\n",
       "      <th>avg_amt_by_cc_num</th>\n",
       "      <th>...</th>\n",
       "      <th>category_GROCERY_POS</th>\n",
       "      <th>category_HEALTH_FITNESS</th>\n",
       "      <th>category_HOME</th>\n",
       "      <th>category_KIDS_PETS</th>\n",
       "      <th>category_MISC_NET</th>\n",
       "      <th>category_MISC_POS</th>\n",
       "      <th>category_PERSONAL_CARE</th>\n",
       "      <th>category_SHOPPING_NET</th>\n",
       "      <th>category_SHOPPING_POS</th>\n",
       "      <th>category_TRAVEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>0.81</td>\n",
       "      <td>-2.34</td>\n",
       "      <td>0.74</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.05</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.11</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    amt  merch_lat  merch_long  cust_lat  cust_long  city_pop  is_fraud   age  \\\n",
       "0 -0.02      -2.30        0.81     -2.34       0.74     -0.21       0.0 -0.20   \n",
       "1  0.14       0.80        0.28      0.95       0.31      0.12       0.0 -1.31   \n",
       "2 -0.14       0.22        1.15      0.31       1.11      4.64       0.0 -0.66   \n",
       "4 -0.12      -0.39       -0.13     -0.58      -0.07     -0.29       0.0 -1.54   \n",
       "5  0.20       0.27       -0.05      0.40      -0.10     -0.29       0.0  0.44   \n",
       "\n",
       "   year_date  avg_amt_by_cc_num  ...  category_GROCERY_POS  \\\n",
       "0       -1.0               1.05  ...                 False   \n",
       "1       -1.0              -0.76  ...                  True   \n",
       "2       -1.0               1.25  ...                 False   \n",
       "4       -1.0               0.21  ...                  True   \n",
       "5       -1.0              -0.70  ...                 False   \n",
       "\n",
       "   category_HEALTH_FITNESS  category_HOME  category_KIDS_PETS  \\\n",
       "0                    False          False               False   \n",
       "1                    False          False               False   \n",
       "2                    False          False               False   \n",
       "4                    False          False               False   \n",
       "5                    False          False               False   \n",
       "\n",
       "   category_MISC_NET  category_MISC_POS  category_PERSONAL_CARE  \\\n",
       "0              False              False                    True   \n",
       "1              False              False                   False   \n",
       "2              False              False                   False   \n",
       "4              False              False                   False   \n",
       "5              False              False                   False   \n",
       "\n",
       "   category_SHOPPING_NET  category_SHOPPING_POS  category_TRAVEL  \n",
       "0                  False                  False            False  \n",
       "1                  False                  False            False  \n",
       "2                  False                  False            False  \n",
       "4                  False                  False            False  \n",
       "5                  False                  False            False  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_df = df.copy(deep=True) # create a deep copy for transformation\n",
    "\n",
    "# Make transformations\n",
    "transformed_df = drop_columns(transformed_df, columns_to_drop=['trans_num', 'first', 'last', 'unix_time'])\n",
    "transformed_df = compute_age(transformed_df, columms_to_compute=['trans_date_trans_time', 'dob'])\n",
    "transformed_df = convert_dates(transformed_df, column_to_convert='trans_date_trans_time')\n",
    "transformed_df = compute_average_columns(transformed_df, columns_to_group=['cc_num', 'merchant', 'street', 'city', 'state', 'zip', 'job'], value_column='amt')\n",
    "transformed_df = frequency_encode_columns(transformed_df, columns_to_encode=['cc_num', 'merchant', 'street', 'city', 'state', 'zip', 'job'])\n",
    "transformed_df = cyclical_encode_columns(transformed_df, columns_to_encode=['day_of_week', 'hour', 'minute', 'seconds', 'day_date', 'month_date'])\n",
    "transformed_df = onehot_encode_columns(transformed_df, columns_to_encode=['sex', 'category'])\n",
    "transformed_df = standardize_columns(transformed_df, columns_to_standardize=[\n",
    "    'amt', 'merch_lat', 'merch_long', 'cust_lat', 'cust_long', 'city_pop', 'age', 'year_date', \n",
    "    'cc_num_freq', 'merchant_freq', 'street_freq', 'city_freq', 'state_freq', 'zip_freq', 'job_freq', \n",
    "    'avg_amt_by_cc_num', 'avg_amt_by_merchant', 'avg_amt_by_street', 'avg_amt_by_city', 'avg_amt_by_state', 'avg_amt_by_zip', 'avg_amt_by_job'])\n",
    "\n",
    "transformed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcacd38a",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32bd704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'is_fraud'\n",
    "features, labels = transformed_df.drop(target_column, axis=1), transformed_df[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d237e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highly_correlated_features(features, threshold = 0.75):\n",
    "    corr_matrix = features.corr().abs() \n",
    "\n",
    "    # Mask self-correlations\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Find pairs above threshold\n",
    "    high_corr = upper.stack()[lambda x: x > threshold].sort_values(ascending=False)\n",
    "\n",
    "    return high_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f5ac1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with high correlation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "avg_amt_by_cc_num  avg_amt_by_street    1.00\n",
       "cc_num_freq        street_freq          1.00\n",
       "merch_long         cust_long            1.00\n",
       "merch_lat          cust_lat             0.99\n",
       "avg_amt_by_cc_num  avg_amt_by_zip       0.99\n",
       "avg_amt_by_street  avg_amt_by_zip       0.99\n",
       "avg_amt_by_city    avg_amt_by_zip       0.98\n",
       "avg_amt_by_cc_num  avg_amt_by_city      0.98\n",
       "avg_amt_by_street  avg_amt_by_city      0.98\n",
       "cc_num_freq        zip_freq             0.95\n",
       "street_freq        zip_freq             0.95\n",
       "avg_amt_by_cc_num  avg_amt_by_job       0.79\n",
       "avg_amt_by_street  avg_amt_by_job       0.79\n",
       "avg_amt_by_zip     avg_amt_by_job       0.78\n",
       "avg_amt_by_city    avg_amt_by_job       0.77\n",
       "city_freq          zip_freq             0.76\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check features correlation\n",
    "high_corr = get_highly_correlated_features(features)\n",
    "print(\"Columns with high correlation\")\n",
    "display(high_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3688b4",
   "metadata": {},
   "source": [
    "Here the correlation is measured to check the linear relationship between features. The result above highlights the columns with a high degree of correlation (>=0.75), which need to be cleaned up because this multicollinearity can make it difficult to determine the individual effect of each feature and result in unstable coefficient estimates.\n",
    "\n",
    "Particularly, the `merch_long` and `merch_lat` are highly correlate to `cust_long` and `cust_lat`, as customers are more likely to make purchase in the same area. The columns `city`, `street` and `zip`  are correlated with each other, since they all represent geographic locations and zipcode mostly aligns with street and city. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5769118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with high correlation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleanups\n",
    "features = drop_columns(features, columns_to_drop=[\n",
    "    'cust_long', 'cust_lat', 'street_freq', 'city_freq', 'zip_freq',\n",
    "    'avg_amt_by_street', 'avg_amt_by_zip', 'avg_amt_by_city', 'avg_amt_by_job'])\n",
    "high_corr = get_highly_correlated_features(features)\n",
    "print(\"Columns with high correlation\")\n",
    "display(high_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eafe9f0",
   "metadata": {},
   "source": [
    "Now the highly correlated features are cleaned and the representative ones are kept, including `cust_long`, `cust_lat` and `cc_num_freq`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f342b9b0",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Based on the findings above, here data transformation is made to establish train and test datasets with standard feature engineering pipeline, to prevent data leakage and ensure data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f387af86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jk/anaconda3/envs/pytorch_gpu_env/lib/python3.12/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalized Features:\n",
      "Index(['sex_F', 'sex_M', 'category_ENTERTAINMENT', 'category_FOOD_DINING',\n",
      "       'category_GAS_TRANSPORT', 'category_GROCERY_NET',\n",
      "       'category_GROCERY_POS', 'category_HEALTH_FITNESS', 'category_HOME',\n",
      "       'category_KIDS_PETS', 'category_MISC_NET', 'category_MISC_POS',\n",
      "       'category_PERSONAL_CARE', 'category_SHOPPING_NET',\n",
      "       'category_SHOPPING_POS', 'category_TRAVEL', 'amt', 'merch_lat',\n",
      "       'merch_long', 'city_pop', 'age', 'year_date', 'cc_num_freq',\n",
      "       'merchant_freq', 'state_freq', 'job_freq', 'avg_amt_by_cc_num',\n",
      "       'avg_amt_by_merchant', 'avg_amt_by_state', 'day_of_week_sin',\n",
      "       'day_of_week_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos',\n",
      "       'seconds_sin', 'seconds_cos', 'day_date_sin', 'day_date_cos',\n",
      "       'month_date_sin', 'month_date_cos'],\n",
      "      dtype='object')\n",
      "X_train shape: (756710, 41); y_train shape: (756710,)\n",
      "X_test shape: (324305, 41); y_test shape: (324305,)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jk/anaconda3/envs/pytorch_gpu_env/lib/python3.12/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get the Train and Test Splits\n",
    "X, y = df.drop('is_fraud', axis=1), df['is_fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=42, stratify=y)   \n",
    "\n",
    "# Feature Enginnering\n",
    "fe = FeatureEngineering()\n",
    "X_train = fe.fit_transform(X_train) # fit only on training set\n",
    "X_test = fe.transform(X_test) # no fit (transform only)\n",
    "\n",
    "assert X_train.shape[1] == X_test.shape[1]\n",
    "print(\"Finalized Features:\")\n",
    "print(X_train.columns)\n",
    "\n",
    "\n",
    "# Convert to Numpy array\n",
    "X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "y_train, y_test = np.array(y_train).ravel().astype(int), np.array(y_test).ravel().astype(int)\n",
    "\n",
    "# Get training and test data shape\n",
    "print(f\"X_train shape: {X_train.shape}; y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}; y_test shape: {y_test.shape}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bfc29c",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45c81ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jk/anaconda3/envs/pytorch_gpu_env/lib/python3.12/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM_linear</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>MLP</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>XGBoost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_time</th>\n",
       "      <td>158.49</td>\n",
       "      <td>8.19</td>\n",
       "      <td>70.00</td>\n",
       "      <td>91.59</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_time</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            SVM_linear  Decision Tree    MLP  Random Forest  XGBoost\n",
       "train_time      158.49           8.19  70.00          91.59     1.22\n",
       "pred_time         0.01           0.02   0.17           1.50     0.06\n",
       "recall            0.63           0.74   0.73           0.58     0.71\n",
       "precision         0.23           0.67   0.78           0.98     0.78\n",
       "f1                0.34           0.70   0.75           0.73     0.74"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = {\n",
    "    'SVM_linear': LinearSVC(dual=True, class_weight='balanced', random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'MLP':  MLPClassifier(random_state=42, max_iter=300),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(eval_metric='logloss'),\n",
    "}\n",
    "\n",
    "scores = defaultdict(list)\n",
    "for model_name, model in models.items():\n",
    "    # Training Time\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "\n",
    "    # Prediction Time\n",
    "    start_pred = time.time()\n",
    "    y_pred = model.predict(X_test)  \n",
    "    end_pred = time.time()\n",
    "\n",
    "    scores[model_name].append(('train_time', end_train-start_train))\n",
    "    scores[model_name].append(('pred_time', end_pred-start_pred))\n",
    "    scores[model_name].append(('recall', recall_score(y_test, y_pred)))\n",
    "    scores[model_name].append(('precision', precision_score(y_test, y_pred)))\n",
    "    scores[model_name].append(('f1', f1_score(y_test, y_pred)))\n",
    "\n",
    "scores_df = pd.DataFrame({model: dict(score) for model, score in scores.items()})\n",
    "display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8f5e8",
   "metadata": {},
   "source": [
    "**Key Insights**\n",
    "| Metric              | Best Model(s)            | Notes                                                                                               |\n",
    "| ------------------- | ------------------------ | --------------------------------------------------------------------------------------------------- |\n",
    "| **Train Time**      | **XGBoost (1.23s)**      | Fastest to train by far.                                                                            |\n",
    "| **Prediction Time** | **SVM (0.01s)**          | Fastest at inference. XGBoost is also fast (0.06s).                                                 |\n",
    "| **Recall**          | **Decision Tree (0.74)** | Best at finding positives. MLP (0.73) and XGBoost (0.71) are close.                                 |\n",
    "| **Precision**       | **Random Forest (0.98)** | Extremely precise, likely predicting fewer positives but with high confidence.                      |\n",
    "| **F1 Score**        | **MLP (0.75)**           | Best balance between precision and recall. XGBoost (0.74) and Random Forest (0.73) are also strong. |\n",
    "\n",
    "\n",
    "Based on the performance results, here are quick takeaways:\n",
    "* XGBoost offers excellent efficiency (fastest training and second-fastest prediction) with strong overall performance.\n",
    "* MLP provides the best F1 score, indicating a great balance, but itâ€™s slower to train and predict.\n",
    "* Random Forest achieves highest precision, but low recall suggests it may be overly conservative.\n",
    "* Decision Tree is simple and recall-focused, with solid precision and fast training.\n",
    "* SVM (linear) performs poorly on precision/F1, despite fast prediction, and has very high training cost.\n",
    "\n",
    "Thus, `XGBoost` is a standout choice for its combination of balanced performance and efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66067da",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d15d8ce",
   "metadata": {},
   "source": [
    "#### Step 1. Find the optimal `n_estimators`\n",
    "The logic is to set `n_estimators` to a large number and let `early_stopping_rounds` find the best iteration automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3576d5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal n_estimators...\n",
      "Optimal n_estimators found: 400\n"
     ]
    }
   ],
   "source": [
    "# Create an evaluation set to use for early stopping\n",
    "eval_set = [(X_test, y_test)]\n",
    "\n",
    "# Instantiate the XGBoost classifier\n",
    "# Pass early_stopping_rounds to the CONSTRUCTOR\n",
    "xgb_model_step1 = xgb.XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=2000,           # Start with a large number, early stopping will find the best\n",
    "    early_stopping_rounds=50,    # Stop if the score doesn't improve for 50 rounds\n",
    "    objective='binary:logistic', # or 'multi:softprob'\n",
    "    eval_metric='logloss',       # or 'mlogloss'\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# The fit method now only needs the eval_set for monitoring\n",
    "print(\"Finding optimal n_estimators...\")\n",
    "xgb_model_step1.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=eval_set,\n",
    "    verbose=False  # Set to True if you want to see the training progress\n",
    ")\n",
    "\n",
    "# Get the optimal number of boosting rounds\n",
    "# The attribute is .best_iteration when early stopping is used\n",
    "optimal_n_estimators = xgb_model_step1.best_iteration\n",
    "print(f\"Optimal n_estimators found: {optimal_n_estimators}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80015451",
   "metadata": {},
   "source": [
    "#### Step 2: Tune Tree-Specific Parameters (max_depth, min_child_weight)\n",
    "Now there is a baseline for `n_estimators`, we can tune the parameters that control tree complexity. RandomizedSearchCV is applied for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6fb7a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best parameters for tree structure: {'max_depth': 7, 'min_child_weight': 1}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'min_child_weight': [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "# Instantiate the model with previously found n_estimators\n",
    "# Note: We are not using early stopping here, so we set n_estimators directly\n",
    "xgb_model_step2 = xgb.XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=optimal_n_estimators, # Use the value from Step 1\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "# Stratified cross-validation strategy \n",
    "cv=StratifiedKFold(n_splits=3, shuffle=True)  \n",
    "\n",
    "# Use GridSearchCV as it's a small search base\n",
    "random_search_step2 = GridSearchCV(\n",
    "    estimator=xgb_model_step2,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1', \n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search_step2.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters for tree structure: {random_search_step2.best_params_}\")\n",
    "\n",
    "# Store the best values\n",
    "best_min_child_weight = random_search_step2.best_params_['min_child_weight']\n",
    "best_max_depth = random_search_step2.best_params_['max_depth']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3513b2",
   "metadata": {},
   "source": [
    "#### Step 3: Tune subsample, colsample_bytree, and gamma\n",
    "With the optimal tree structure set, we can now tune the parameters that add randomness and conservatism to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4841cbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Best Gamma, Subsample, and Colsample_bytree Params: {'subsample': 0.8, 'gamma': 0.4, 'colsample_bytree': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for this step\n",
    "# We are tuning the conservatism (gamma) and sampling (subsample, colsample_bytree)\n",
    "param_grid_step3 = {\n",
    "    'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Instantiate the model with all previously tuned parameters\n",
    "xgb_model_step3 = xgb.XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=optimal_n_estimators,\n",
    "    max_depth=best_max_depth,\n",
    "    min_child_weight=best_min_child_weight,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Stratified cross-validation strategy \n",
    "cv=StratifiedKFold(n_splits=3, shuffle=True) \n",
    "\n",
    "# Use RandomizedSearchCV for efficiency\n",
    "grid_search_step3 = RandomizedSearchCV(\n",
    "    estimator=xgb_model_step3,\n",
    "    param_distributions=param_grid_step3,\n",
    "    n_iter=100, # Try 100 different random combinations\n",
    "    random_state=42,\n",
    "    scoring='f1', # Or your preferred metric\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Remember to use your encoded labels if you performed that step\n",
    "grid_search_step3.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Gamma, Subsample, and Colsample_bytree Params: {grid_search_step3.best_params_}\")\n",
    "\n",
    "# Store the best values to use in the next steps\n",
    "best_gamma = grid_search_step3.best_params_['gamma']\n",
    "best_subsample = grid_search_step3.best_params_['subsample']\n",
    "best_colsample_bytree = grid_search_step3.best_params_['colsample_bytree']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4d30a",
   "metadata": {},
   "source": [
    "#### Step 4: Tuning Regularization Parameters (reg_alpha, reg_lambda)\n",
    "In this step, we will fine-tune the L1 and L2 regularization terms to reduce overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f263ecb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "Best Regularization Params: {'reg_alpha': 0.05, 'reg_lambda': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for regularization\n",
    "param_grid_step4 = {\n",
    "    'reg_alpha': [0, 0.001, 0.005, 0.01, 0.05],\n",
    "    'reg_lambda': [0.5, 1.0, 1.5, 2.0, 3.0]\n",
    "}\n",
    "\n",
    "# Instantiate the model with all previously tuned parameters\n",
    "xgb_model_step4 = xgb.XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=optimal_n_estimators,\n",
    "    max_depth=best_max_depth,\n",
    "    min_child_weight=best_min_child_weight,\n",
    "    gamma=best_gamma,\n",
    "    subsample=best_subsample,\n",
    "    colsample_bytree=best_colsample_bytree,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Stratified cross-validation strategy \n",
    "cv=StratifiedKFold(n_splits=3, shuffle=True) \n",
    "\n",
    "# Use GridSearchCV to find the best regularization parameters\n",
    "grid_search_step4 = GridSearchCV(\n",
    "    estimator=xgb_model_step4,\n",
    "    param_grid=param_grid_step4,\n",
    "    scoring='f1', # Or your preferred metric\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Remember to use your encoded labels if you performed that step\n",
    "grid_search_step4.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Regularization Params: {grid_search_step4.best_params_}\")\n",
    "\n",
    "# Store the best values\n",
    "best_reg_alpha = grid_search_step4.best_params_['reg_alpha']\n",
    "best_reg_lambda = grid_search_step4.best_params_['reg_lambda']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df181d3a",
   "metadata": {},
   "source": [
    "#### Step 5: Lower the Learning Rate and Re-evaluate n_estimators\n",
    "This is the final and crucial step. With all other parameters optimized, we lower the `learning_rate` to make the model more robust and re-calculate the final, optimal number of trees using early stopping. A lower learning rate will almost always require more trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acec61bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Model ---\n",
      "Final optimal values for tuned params:\n",
      "n_estimators(with low learning rate)=1066\n",
      "max_depth=7\n",
      "min_child_weight=1\n",
      "gamma=0.4\n",
      "subsample=0.8\n",
      "colsample_bytree=1.0\n",
      "reg_alpha=0.05\n",
      "reg_lambda=1.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    322894\n",
      "           1       0.96      0.76      0.85      1411\n",
      "\n",
      "    accuracy                           1.00    324305\n",
      "   macro avg       0.98      0.88      0.92    324305\n",
      "weighted avg       1.00      1.00      1.00    324305\n",
      "\n",
      "AUC-ROC Score: 0.9987\n",
      "train_time: 18.441873788833618\n",
      "pred_time: 0.7462122440338135\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the final model with all tuned parameters and a lower learning rate\n",
    "final_xgb_model = xgb.XGBClassifier(\n",
    "    # --- Tuned parameters from previous steps ---\n",
    "    max_depth=best_max_depth,\n",
    "    min_child_weight=best_min_child_weight,\n",
    "    gamma=best_gamma,\n",
    "    subsample=best_subsample,\n",
    "    colsample_bytree=best_colsample_bytree,\n",
    "    reg_alpha=best_reg_alpha,\n",
    "    reg_lambda=best_reg_lambda,\n",
    "\n",
    "    # --- Final adjustments ---\n",
    "    learning_rate=0.02, # Use a lower learning rate\n",
    "    n_estimators=5000,  # Use a very large number of estimators\n",
    "    early_stopping_rounds=50, # Early stopping will find the true optimal number\n",
    "\n",
    "    # --- Other fixed parameters ---\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the final model to find the new optimal n_estimators\n",
    "start_train = time.time()\n",
    "final_xgb_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "end_train = time.time()\n",
    "\n",
    "print(\"\\n--- Final Model ---\")\n",
    "print(\"Final optimal values for tuned params:\")\n",
    "print(f\"n_estimators(with low learning rate)={final_xgb_model.best_iteration}\")\n",
    "print(f\"max_depth={best_max_depth}\")\n",
    "print(f\"min_child_weight={best_min_child_weight}\")\n",
    "print(f\"gamma={best_gamma}\")\n",
    "print(f\"subsample={best_subsample}\")\n",
    "print(f\"colsample_bytree={best_colsample_bytree}\")\n",
    "print(f\"reg_alpha={best_reg_alpha}\")\n",
    "print(f\"reg_lambda={best_reg_lambda}\")\n",
    "\n",
    "# Evaluate the performance on test data\n",
    "start_test = time.time()\n",
    "y_pred = final_xgb_model.predict(X_test)\n",
    "end_test = time.time()\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "y_proba = final_xgb_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"AUC-ROC Score: {roc_auc:.4f}\")\n",
    "\n",
    "print(f\"train_time: {end_train - start_train}\")\n",
    "print(f\"pred_time: {end_test - start_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c4f3eb",
   "metadata": {},
   "source": [
    "**Key Insights**\n",
    "\n",
    "* Significant Performance Boost: You have successfully improved upon your already strong baseline. The F1-score for the positive class (class 1) jumped from 0.82 to 0.86, which is a substantial gain and the primary indicator of a successful tuning process.\n",
    "\n",
    "* Drastic Increase in Precision: The most impressive gain is in precision, which leaped from 0.88 to 0.96. This means your final model is extremely reliable; when it predicts the positive class, it is correct 96% of the time. This is a critical improvement, especially if the cost of a false positive is high.\n",
    "\n",
    "* Outstanding Discriminatory Power: An AUC-ROC score of 0.9990 is nearly perfect. This indicates that your model has an almost flawless ability to distinguish between the positive (1) and negative (0) classes.\n",
    "\n",
    "* Handling Class Imbalance Well: The classification report shows a significant class imbalance (322,894 negative samples vs. 1,411 positive samples). The high scores for the minority class (class 1) and the near-perfect AUC score confirm that your model has learned the patterns of the minority class effectively, rather than just defaulting to the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d6f1112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff9b96",
   "metadata": {},
   "source": [
    "The previous test set results (Precision: 0.96, Recall: 0.78, F1: 0.86) are located in an excellent position on this curve, near the optimal balance point (the \"Elbow\" part). This demonstrates confidence that it is a robust and well-calibrated model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
